# Deep Learning

This repository contains all the six homeworks completed for the Deep Learning 2020/2021 course atended at Universit√† degli Studi di Padova.

## From Perceptron to DNN
In this homework, we are going to write our own simple feedforward neural network using **Python** and **NumPy**. We will start by implementing just a simple neuron, or perceptron, and then we define the training algorithm for this simple model. The second part consists in defining a simple neural network to perform digits classification.

## Optimize and train Deep Models
In this homework, we are going to see how to develop a simple deep neural network (**DNN**) for a classification problem. We will explore two common libraries: **TensorFlow** and **Keras**. Then we will explore how to face a well known problem that is common to encounter during the training phase: the *overfitting*. Finally, we will study how to perform a fair model selection.

## Convolutional Neural Networks
In this homework, we will explore how to develop a simple convolutional neural network (**CNN**) for image classification. We will use the *Fashion Minst* dataset. In the first part, we will learn how to develop a simple CNN, while in the second part we will explore the impact of various hyper-parameters in the learning performances.

## Recurrent Neural Networks
In this homework, we will explore how to develop a simple recurrent neural network (**RNN**) for sentiment analysis. As a dataset, we will use the *IMDB* dataset. The input to the RNN is the sequence of words that compose a review. The learning task consists in predicting the sentiment of the review. In the first part, we will learn how to develop a simple RNN, then we will explore the differences in terms of computational load, number of parameters, and performances with respect to more advanced recurrent models, like **LSTM** and **GRU**. Subsequently, we experiment with the bi-directional model to unveil the strengths and the weaknesses of this technique. Finally, we will explore how to face overfitting by *Dropout*.

## Autoencoders
In this homework, we will explore how to develop a simple autoencoder. We will use *MNIST* as dataset. In the first part, we will learn how to develop a simple shallow autoencoder, then we will develop a deep version. Next, we will experiment with the application of autoencoder on denoising data task (denoising-autoencoder). Finally, we will apply this model to sequential domains considering the *IMDB* dataset.

## Variational Autoencoders
In this homework, we will explore how to develop a variational autoencoder (**VAE**). We will use *MNIST* as dataset. In developing the VAE we also explore how to develop an ad-hoc layer and a non-standard training step.
